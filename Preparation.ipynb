{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run R in Jupyter Notebook\n",
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('.env')\n",
    "\n",
    "API_KEY = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "%R -i API_KEY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preparation for Air Pollution Index 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# Data Preparation for Air Pollution Index 2017\n",
    "\n",
    "API = read.csv(\"RawDataSet\\\\API.csv\",header=TRUE)       # Read the data\n",
    "API$Time = strptime(API$Time, format = \"%d-%m-%y %H:%M\")    # Change the format of Time\n",
    "API$Time = format(API$Time, \"%Y\")                                               # Change the time attribute only the year\n",
    "API$Time = as.integer(API$Time)                                                     # Change the format of Time to integer\n",
    "API = as.data.frame(lapply(API, as.integer))                                      # I also don't know why I need to do this\n",
    "\n",
    "API_17 = subset(API, Time == 2017)                    # Subset the data to only 2017\n",
    "\n",
    "#! To check for number of missing values, Minden (supposely padang tengky) has the most missing values beside completely missing values - 6207 missing out of 8760 available data = 71% missing\n",
    "na_counts = colSums(is.na(API_17))\n",
    "na_df = data.frame(Column = names(na_counts), NA_Count = na_counts)\n",
    "\n",
    "\n",
    "API_17 = round(colMeans(API_17, na.rm = TRUE), digits = 0)   # Calculate the mean of each column\n",
    "API_17 = data.frame(API_17)\n",
    "\n",
    "#geocode\n",
    "library(ggmap)\n",
    "register_google(key = API_KEY)\n",
    "\n",
    "hold_coor = data.frame()\n",
    "hold_coors = data.frame()\n",
    "\n",
    "for (i in 2:nrow(API_17)) {\n",
    "  location = paste(rownames(API_17)[i], \", Malaysia\")\n",
    "  coor = geocode(location = location, output = \"latlon\")\n",
    "  hold_coor = rbind(hold_coor, cbind(location, coor))\n",
    "}\n",
    "\n",
    "API_17 = data.frame(API_17[-1,]) #remove the year row\n",
    "hold_coors = cbind(hold_coor, API_17)\n",
    "colnames(hold_coors) = c(\"location\", \"lon\", \"lat\", \"API\")\n",
    "\n",
    "  # remove east malaysia\n",
    "hold_coors = subset(hold_coors, hold_coors$lon < 105)\n",
    "write.csv(hold_coors, file = \"PreparedDataSet\\\\API_17.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%R -o na_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preparation for Air Pollution Index 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# Data Preparation for Air Pollution Index 2018\n",
    "\n",
    "API = read.csv(\"RawDataSet\\\\API.csv\",header=TRUE)       # Read the data\n",
    "API$Time = strptime(API$Time, format = \"%d-%m-%y %H:%M\")    # Change the format of Time\n",
    "API$Time = format(API$Time, \"%Y\")                                               # Change the time attribute only the year\n",
    "API$Time = as.integer(API$Time)                                                     # Change the format of Time to integer\n",
    "API = as.data.frame(lapply(API, as.integer))                                      # I also don't know why I need to do this\n",
    "\n",
    "API_18 = subset(API, Time == 2018)                    # Subset the data to only 2018\n",
    "\n",
    "na_counts = colSums(is.na(API_18))\n",
    "na_df = data.frame(Column = names(na_counts), NA_Count = na_counts)\n",
    "\n",
    "# API_18 = round(colMeans(API_18, na.rm = TRUE), digits = 0)\n",
    "# API_18 = data.frame(API_18)\n",
    "\n",
    "# #geocode\n",
    "# library(ggmap)\n",
    "# register_google(key = API_KEY)\n",
    "\n",
    "# hold_coor = data.frame()\n",
    "# hold_coors = data.frame()\n",
    "\n",
    "# for (i in 2:nrow(API_18)) {\n",
    "#   location = paste(rownames(API_18)[i], \", Malaysia\")\n",
    "#   coor = geocode(location = location, output = \"latlon\")\n",
    "#   hold_coor = rbind(hold_coor, cbind(location, coor))\n",
    "# }\n",
    "\n",
    "# API_18 = data.frame(API_18[-1,]) #remove the year row\n",
    "# hold_coors = cbind(hold_coor, API_18)\n",
    "# colnames(hold_coors) = c(\"location\", \"lon\", \"lat\", \"API\")\n",
    "\n",
    "#   # remove east malaysia\n",
    "# hold_coors = subset(hold_coors, hold_coors$lon < 105)\n",
    "# write.csv(hold_coors, file = \"PreparedDataSet\\\\API_18.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Prepation for API 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code performs data preparation for the Air Pollution Index (API) dataset for the year 2019.\n",
    "It reads the raw dataset, changes the format of the time attribute, subsets the data for the year 2019,\n",
    "calculates the mean API values, geocodes the locations, removes certain locations, and writes the prepared dataset to a CSV file.\n",
    "\"\"\"\n",
    "\n",
    "%%R\n",
    "# Data Preparation for Air Pollution Index 2019\n",
    "\n",
    "API = read.csv(\"RawDataSet\\\\API.csv\",header=TRUE)       # Read the data\n",
    "API$Time = strptime(API$Time, format = \"%d-%m-%y %H:%M\")    # Change the format of Time\n",
    "API$Time = format(API$Time, \"%Y\")                                               # Change the time attribute only the year\n",
    "API$Time = as.integer(API$Time)                                                     # Change the format of Time to integer\n",
    "API = as.data.frame(lapply(API, as.integer))                                      # I also don't know why I need to do this\n",
    "\n",
    "API_19 = subset(API, Time == 2019)                    # Subset the data to only 2019\n",
    "API_19 = round(colMeans(API_19, na.rm = TRUE), digits = 0)\n",
    "API_19 = data.frame(API_19)\n",
    "\n",
    "#geocode\n",
    "library(ggmap)\n",
    "register_google(key = API_KEY)\n",
    "\n",
    "hold_coor = data.frame()\n",
    "hold_coors = data.frame()\n",
    "\n",
    "for (i in 2:nrow(API_19)) {\n",
    "  location = paste(rownames(API_19)[i], \", Malaysia\")\n",
    "  coor = geocode(location = location, output = \"latlon\")\n",
    "  hold_coor = rbind(hold_coor, cbind(location, coor))\n",
    "}\n",
    "\n",
    "API_19 = data.frame(API_19[-1,]) #remove the year row\n",
    "hold_coors = cbind(hold_coor, API_19)\n",
    "colnames(hold_coors) = c(\"location\", \"lon\", \"lat\", \"API\")\n",
    "\n",
    "# remove east malaysia & langkawi\n",
    "hold_coors = subset(hold_coors, hold_coors$lon < 105)\n",
    "hold_coors = subset(hold_coors, hold_coors$lon > 100)\n",
    "hold_coors = na.omit(hold_coors) # remove NA values\n",
    "write.csv(hold_coors, file = \"PreparedDataSet\\\\API_19.csv\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meteo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "library(dplyr)\n",
    "Meteo_17 = read.csv(\"RawDataSet\\\\Meteo2017.csv\")\n",
    "Meteo_17 = Meteo_17 %>% group_by(name, longitude, latitude) %>% summarize(temp = mean(temp), humidity=mean(humidity),precip=mean(precip),humidity=mean(humidity), windspeed = mean(windspeed))\n",
    "write.csv(Meteo_17, file = \"PreparedDataSet\\\\Meteo_17.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "library(dplyr)\n",
    "Meteo_18 = read.csv(\"RawDataSet\\\\Meteo2018.csv\")\n",
    "Meteo_18 = Meteo_18 %>% group_by(name, longitude, latitude) %>% summarize(temp = mean(temp), humidity=mean(humidity),precip=mean(precip),humidity=mean(humidity), windspeed = mean(windspeed))\n",
    "write.csv(Meteo_18, file = \"PreparedDataSet\\\\Meteo_18.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "library(dplyr)\n",
    "Meteo_19 = read.csv(\"RawDataSet\\\\Meteo2019.csv\")\n",
    "Meteo_19 = Meteo_19 %>% group_by(name, longitude, latitude) %>% summarize(temp = mean(temp), humidity=mean(humidity),precip=mean(precip),humidity=mean(humidity), windspeed = mean(windspeed))\n",
    "write.csv(Meteo_19, file = \"PreparedDataSet\\\\Meteo_19.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vegetation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "library(dplyr)\n",
    "Vegetation_171819 = read.csv(\"RawDataSet\\\\vegetation.csv\")\n",
    "\n",
    "Vegetation_171819 = subset(Vegetation_171819, Vegetation_171819$threshold == 30)         #select only 30% tree cover\n",
    "Vegetation_171819 = Vegetation_171819 %>%\n",
    "            mutate(treecover2017 = extent_2010_ha - rowSums(.[, 19:25], na.rm = TRUE) + gain_2000.2020_ha) %>%\n",
    "            mutate(treecover2018 = extent_2010_ha - rowSums(.[, 19:26], na.rm = TRUE) + gain_2000.2020_ha) %>%\n",
    "            mutate(treecover2019 = extent_2010_ha - rowSums(.[, 19:27], na.rm = TRUE) + gain_2000.2020_ha) %>%\n",
    "            select(subnational1,subnational2, treecover2017, treecover2018, treecover2019)\n",
    "            \n",
    "\n",
    "#geocode\n",
    "library(ggmap)\n",
    "register_google(key = API_KEY)\n",
    "\n",
    "hold_coor = data.frame()\n",
    "hold_coors = data.frame()\n",
    "for (i in 1:nrow(Vegetation_171819)) {\n",
    "  location = paste(Vegetation_171819[i,2], \",\", Vegetation_171819[i,1],\", Malaysia\")\n",
    "  coor = geocode(location = location, output = \"latlon\")\n",
    "  hold_coor = rbind(hold_coor, cbind(location, coor))\n",
    "}\n",
    "hold_coors = cbind(hold_coor,Vegetation_171819[,3:5])\n",
    "#remove east malaysia & langkawi\n",
    "hold_coors = subset(hold_coors, hold_coors$lon < 105)\n",
    "hold_coors = subset(hold_coors, hold_coors$lon > 100)\n",
    "write.csv(hold_coors,file=\"PreparedDataSet\\\\Vegetation_171819.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "Traffic_171819 = pd.read_csv(\"RawDataSet\\\\Traffic.csv\")\n",
    "Traffic_171819 = Traffic_171819.drop(['BIL','STESEN','2013','2014','2015','2016','2020','2021','2022'], axis = 1)\n",
    "Traffic_171819['LOKASI'] = Traffic_171819['LOKASI'].str.split('-')\n",
    "Traffic_171819 = Traffic_171819.explode('LOKASI').reset_index(drop = True)\n",
    "Traffic_171819['2017'] = Traffic_171819['2017'].str.replace(',','')\n",
    "Traffic_171819['2018'] = Traffic_171819['2018'].str.replace(',','')\n",
    "Traffic_171819['2019'] = Traffic_171819['2019'].str.replace(',','')\n",
    "Traffic_171819['2017'] = Traffic_171819['2017'].astype(float)\n",
    "Traffic_171819['2018'] = Traffic_171819['2018'].astype(float)\n",
    "Traffic_171819['2019'] = Traffic_171819['2019'].astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%R -i Traffic_171819 # Pass the data frame to R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# geocode\n",
    "library(ggmap)\n",
    "register_google(key = API_KEY)\n",
    "\n",
    "hold_coor = data.frame()\n",
    "hold_coors = data.frame()\n",
    "for (i in 1:nrow(Traffic_171819)) {\n",
    "  location = paste(Traffic_171819[i,1],\", Malaysia\")\n",
    "  coor = geocode(location = location, output = \"latlon\")\n",
    "  hold_coor = rbind(hold_coor, cbind(location, coor))\n",
    "}\n",
    "\n",
    "hold_coors = cbind(hold_coor,Traffic_171819[,2:4])\n",
    "write.csv(hold_coors,file=\"PreparedDataSet\\\\Traffic_171819.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
